{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb98c0c-1290-46ab-abed-9f963cd94150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow[and-cuda]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43471d26-d46a-4abc-aac6-e5e55f0bfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import LSTM, Dense, GlobalAveragePooling2D, TimeDistributed, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497ac87a-4dfa-45df-b6c2-88ad019aee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, max_frames=10, target_size=(224,224)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = max(1, total_frames // max_frames)\n",
    "    \n",
    "    for i in range(max_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b108698-fc08-47b9-9fd6-fdc20cb5d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elvisha\\OneDrive\\Desktop\\PBL\\Dataset -> 0 files\n",
      "C:\\Users\\Elvisha\\OneDrive\\Desktop\\PBL\\Dataset\\NonViolence -> 25 files\n",
      "C:\\Users\\Elvisha\\OneDrive\\Desktop\\PBL\\Dataset\\Violence -> 25 files\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = r\"C:\\Users\\Elvisha\\OneDrive\\Desktop\\PBL\\Dataset\"\n",
    "\n",
    "for root, dirs, files in os.walk(DATASET_DIR):\n",
    "    print(root, \"->\", len(files), \"files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f0d9c5-b604-453d-8b31-8a9fc5368177",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"NonViolence\", \"Violence\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614919ec-8176-43f8-b301-834c0fabf085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset loaded successfully\n",
      "X shape: (50, 10, 224, 224, 3)\n",
      "y shape: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "def extract_frames(video_path, max_frames=10, target_size=(224,224)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = max(1, total_frames // max_frames)\n",
    "    \n",
    "    for i in range(max_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "for label, category in enumerate(categories):\n",
    "    folder = os.path.join(DATASET_DIR, category)\n",
    "    for video in os.listdir(folder):\n",
    "        video_path = os.path.join(folder, video)\n",
    "        try:\n",
    "            frames = extract_frames(video_path)\n",
    "            if len(frames) == 10:\n",
    "                X.append(frames)\n",
    "                y.append(label)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping:\", video_path, \"due to\", e)\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(np.array(y))\n",
    "print(\" Dataset loaded successfully\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d17621e-b1e4-4cb1-a745-39408ed5dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f19f466-90c0-49e5-8ad6-ffd3ac66fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv3D(32, kernel_size=(3,3,3), activation='relu', input_shape=(10,224,224,3)),\n",
    "    MaxPooling3D(pool_size=(1,2,2)),\n",
    "    Conv3D(64, kernel_size=(3,3,3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2,2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafafbdd-8239-48ec-9d0e-3d93c9597c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, LSTM\n",
    "\n",
    "# Use a pre-trained CNN (like MobileNetV2) inside TimeDistributed for feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e3b92b-d43c-4dd3-8006-18f7bd6de260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 4.7073 - accuracy: 0.5278 - val_loss: 0.6761 - val_accuracy: 0.2500\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 0.6371 - accuracy: 0.6111 - val_loss: 0.6663 - val_accuracy: 0.2500\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.6176 - accuracy: 0.7778 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.5604 - accuracy: 0.7500 - val_loss: 0.5217 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 111s 6s/step - loss: 0.5191 - accuracy: 0.7222 - val_loss: 0.5289 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.4685 - accuracy: 0.7500 - val_loss: 0.4721 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 111s 6s/step - loss: 0.6957 - accuracy: 0.6667 - val_loss: 0.1392 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.3684 - accuracy: 0.8611 - val_loss: 0.3657 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 1.3500 - accuracy: 0.8333 - val_loss: 0.4596 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 97s 5s/step - loss: 0.3803 - accuracy: 0.8056 - val_loss: 0.5516 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 94s 5s/step - loss: 0.2787 - accuracy: 0.8889 - val_loss: 0.3918 - val_accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 0.1830 - accuracy: 0.8889 - val_loss: 0.2163 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.1395 - accuracy: 0.9444 - val_loss: 0.1182 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 0.0734 - accuracy: 1.0000 - val_loss: 2.8407 - val_accuracy: 0.2500\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 2.0062 - accuracy: 0.8611 - val_loss: 0.4626 - val_accuracy: 0.7500\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 97s 5s/step - loss: 1.4784 - accuracy: 0.7500 - val_loss: 0.4347 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 97s 5s/step - loss: 0.8363 - accuracy: 0.6944 - val_loss: 0.4309 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 0.4997 - accuracy: 0.7778 - val_loss: 0.2148 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 94s 5s/step - loss: 0.5083 - accuracy: 0.6944 - val_loss: 0.3937 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.4211 - accuracy: 0.8056 - val_loss: 0.3845 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=20,\n",
    "    batch_size=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f149bcbd-4dde-479c-aa4a-27ee461e8a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.5244 - accuracy: 0.7000\n",
      "Test accuracy: 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a713bc-c94e-409e-9ed7-0e2c28ef1064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 495ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 310ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "for i in range(3):  # visualize 3 test videos\n",
    "    frames = X_test[i]\n",
    "    pred_class = categories[np.argmax(model.predict(frames[np.newaxis]))]\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame_show = (frame * 255).astype(np.uint8)\n",
    "        cv2.putText(frame_show, pred_class, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Frame', frame_show)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f5c3849-f740-4ce8-b738-896b14cd7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 283ms/step\n",
      "Violence Probability: 0.97\n",
      "⚠️ Fight Detected!\n"
     ]
    }
   ],
   "source": [
    "def predict_video(video_path, visualize=False):\n",
    "    frames = extract_frames(video_path)\n",
    "    frames_input = np.expand_dims(frames, axis=0)\n",
    "    \n",
    "    pred = model.predict(frames_input)[0]\n",
    "    violence_prob = pred[0]\n",
    "    \n",
    "    if violence_prob > 0.6:\n",
    "        alert = \"⚠️ Fight Detected!\"\n",
    "    else:\n",
    "        alert = \"✅ No Fight Detected\"\n",
    "    \n",
    "    print(f\"Violence Probability: {violence_prob:.2f}\")\n",
    "    print(alert)\n",
    "    \n",
    "    if visualize:\n",
    "        import cv2\n",
    "        for frame in frames:\n",
    "            frame_show = (frame * 255).astype(np.uint8)\n",
    "            cv2.putText(frame_show, alert, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Video Prediction', frame_show)\n",
    "            if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    return alert\n",
    "\n",
    "# Example usage\n",
    "predict_video(\"crowd_violence__Occupy_Cal_video_Police_brutally_beat_arrest_Berkeley_students__RussiaToday__B_f06VQOkI4.avi\", visualize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6c49323-5ce6-4172-b3f7-205d4a5e1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELVISHA\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"fight_detection_model.h5\")  # .h5 format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d98ef-ac1e-48e1-ad01-e6741b394d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
